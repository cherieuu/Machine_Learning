# GBM (Gradient Boosting Machine)

- 여러 개의 약한 학습기(weak learner)를 순차적으로 학습-예측하면서
  잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가며 학습하는 방식

### AdaBoost

**adaptive + boosting**

간단한 **약분류기(week classifier)**들이 상호보완 하도록 단계적으로 학습,
이들을 조합해 최종 강 분류기의 성능을 증폭

### Gradient Boost

- 방식 자체는 Adaboost와 비슷하나 가중치 업데이트를 `경사하강법` 을 이용한다.
- 경사하강법은 반복수행을 통해 오류를 최소화할 수 있도록 가중치의 업데이트 값을 도출하는 기법이다.

**Gradient Boost 과정**

**1. 최소, 평균값으로 예측**

**2. 예측값과 실제 값의 오차 구하기**

**3. 오차 값을 예측하는 Tree 만들기** 

- 분류된 값이 여러개가 있는 경우, 평균을내 하나의 값으로 대체한다.

**4. Learning rate 적용하여, 기존 예측 값 업데이트**

- 오차 값을 원래 예측 값에 더해주어 원래 예측 값을 업데이트 시켜주려한다.
- 다만, 그냥 더해주면 첫번째 행을 기준으로 71.2 + 16.8 = 88 로 원래 Weight 값과 동일하다.
- 이런 경우, 기존 데이터셋에는 완벽하게 학습되겠지만 새로운 데이터셋에는 잘 맞지 않을 가능성이 높다.
- 즉, 모델이 과적합된다.
- 그래서 그대로 더하지 않고 Learning rate 만큼만 더한다.
- Learning rate 값은 0.1로 한것으로 한다.

# XGBoost (eXtra Gradient Boost)

### 개요

- 트리 기반 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나.

- 압도적인 수치는 아니지만 다른 머신러닝보다 뛰어난 예측 성능을 가진다.
- GBM에 기반을 두고 있지만, GBM의 단점을 해결한 알고리즘이다.
- 특히, GPU 환경에서 병렬 학습이 가능해 속도가 기존 GBM보다 매우 빠르다.

### XGboost의 장점

- **뛰어난 예측 성능**
  - 분류와 회귀 영역에서 뛰어난 예측 성능을 보인다.
- **GBM 대비 빠른 수행 시간**
  - GBM은 순차적으로 약한 학습기가 가중치를 증감하는 방법으로 학습하기에 속도가 전반적으로 느리다.
  - xgboost는 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행 성능을 보인다
    (다만 다른 머신러닝 알고리즘보다 빠르다는것은 아니다)
- **과적합 규제 (Regularization)**
  - 과적합에 대해 내구성이 강하다.
- **나무 가지치기**
  - GBM은 일반적으로 분할 시 부정 손실이 발생하면 분할을 더 이상 수행하지 않는다.
  - xgboot는 긍정 이득이 없는 분할을 가지치기해 분할 수를 줄일 수 있다.
- **자체 내장 교차 검증**
  - xgboost는 반복 수행 시마다 내부적으로 학습 데이터와 평가 데이터를 교차 검증해 최적화된 반복 수행 횟수를 가질 수 있다.
  - 평가 값이 최적화 되면 반복을 조기 중단할 수 있는 기능이 있다.
- **결손값 자체 처리**
  - xgboost는 결손값을 자체 처리할 수 있는 기능이 있다.

### XGBoost 하이퍼 파라미터

**민감하게 조정해야하는 것**

- booster 모양
- eval_metric(평가함수) / objective(목적함수)
- eta
- L1 form (L1 레귤러라이제이션 폼이 L2보다 아웃라이어에 민감)
- L2 form

**과적합 방지를 위해 조정해야하는 것**

- learning rate 낮추기 → n_estimators은 높여야함
- max_depth 낮추기
- min_child_weight 높이기
- gamma 높이기
- subsample, colsample_bytree 낮추기

### Early Stopping

- 조기 중단 기능
- 기존 GBM은 지정된 횟수만큼 반복적으로 학습 오류를 감소시키며 학습을 진행하면서
  중간에 반복을 멈출 수 없고 n_estimator에 지정된 횟수를 다 완료해야 한다.
- 하지만 xgboot는 조기 중단 기능이 있어 예측 오류가 더 이상 개선되지 않으면 중지한다.

# 회귀 

트렌드, 경향, 연속된 값 예측할 때 사용